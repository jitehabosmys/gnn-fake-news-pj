# 参考链接：https://github.com/safe-graph/GNN-FakeNews
数据获取：https://codeocean.com/capsule/7305473/tree/v1 的data/，下载后放在项目data/

## 题目要求

### 课题3：欺诈检测

#### 任务说明参考
1. **可视化用户之间的社交关系。**
2. **根据用户社交关系、发帖/传播行为、帖子内容等特征，设计一种机器学习模型来识别社交网络中的虚假信息（谣言）、僵尸（虚假）用户，并说明模型的评价方法和识别效果。**
3. **分析僵尸用户和正常用户的社交网络，或真实信息与谣言的传播网络的异同。**

> **补充说明：** 若将一个邮箱地址对应为一个用户，则该任务变成垃圾邮件/邮箱检测。

#### 参考数据集

> *以上所有课题任务皆可自行爬取数据，或从eLearning上参考文献（后续更新）中提供的下载链接下载数据集，相关模型、算法亦可借鉴参考文献。*

#### 附：其他参考资料

- **可视化工具**：Gephi、Echarts、Highcharts、D3、Pajek、NetworkX 等。

---

## 数据集与模型介绍

好的，这个仓库（GNN-FakeNews）是一个基于图神经网络（GNN）的假新闻检测模型合集。下面我将从数据集形式、模型内容以及如何参考使用这三个方面为你详细解读。

### 1. 数据集形式：User Preference-aware Fake News Detection (UPFD)

这个项目的核心是提出了一个名为 **UPFD** 的假新闻检测框架和相应的数据集。

#### 基本概念：
- **任务定义**： 将假新闻检测问题定义为一个**图分类任务**。即，输入一个图，模型需要判断这个图代表的新闻是“真”还是“假”。
- **图的结构**： 每个图都是一棵以新闻为中心的传播树。
    - **根节点**： 代表一条新闻。
    - **叶子节点**： 代表在Twitter上转发了这条新闻的用户。
    - **边**： 有两种类型：
        1.  用户和新闻之间的边：表示用户转发了该新闻。
        2.  用户和用户之间的边：表示用户A转发了用户B关于这条新闻的推文（即A是通过B看到这条新闻的）。

#### 数据集详情：
- **来源**： 基于两个知名的假新闻核查平台构建：
    - **Politifact**： 政治新闻领域。
    - **Gossipcop**： 娱乐八卦新闻领域。
- **规模**：

    | 数据集 | 图数量 | 假新闻数量 | 总节点数 | 总边数 | 平均每图节点数 |
    | :--- | :--- | :--- | :--- | :--- | :--- |
    | Politifact | 314 | 157 | 41,054 | 40,740 | 131 |
    | Gossipcop | 5,464 | 2,732 | 314,262 | 308,798 | 58 |

- **节点特征**： 提供了四种类型的用户节点特征：
    1.  **bert (768维)**： 使用预训练的BERT模型编码的用户的历史200条推文。
    2.  **spacy (300维)**： 使用spaCy的word2vec模型编码用户的历史200条推文。
    3.  **profile (10维)**： 从Twitter用户个人资料中提取的特征（如粉丝数、注册时间等）。
    4.  **content (310维)**： `spacy`特征和`profile`特征的组合，但spacy编码的是**用户当前转发的评论**，而不包括历史推文。

#### 数据获取与使用：
- **便捷方式**： UPFD数据集已经被集成到两个主流的图深度学习库中：
    - **PyTorch Geometric (PyG)**： 可以直接使用`torch_geometric.datasets.UPFD`来加载。
    - **Deep Graph Library (DGL)**： 同样有内置的数据加载器。
    这是最推荐的方式，因为它自动处理了数据下载和预处理。
- **手动下载**： 如果手动安装本仓库，需要从提供的Google Drive链接下载数据，并解压到指定目录。

### 2. 模型

这个仓库实现了多种基于GNN的图分类模型，所有模型代码都在 `/gnn_model` 目录下。

**核心模型列表**：

1.  **GNN-CL**： 结合了持续学习，旨在解决模型在新数据上学习时的“灾难性遗忘”问题。
2.  **GCNFN**： 专门为假新闻检测设计的图卷积网络模型。
3.  **BiGCN**： 双向图卷积网络，同时捕捉新闻传播的“自顶向下”（从新闻到用户）和“自底向上”（从用户到新闻）的模式。
4.  **UPFD-GCN**： 标准的图卷积网络，作为强大的基线模型。
5.  **UPFD-GAT**： 图注意力网络，可以学习节点之间关系的重要性权重。
6.  **UPFD-SAGE**： 图采样与聚合模型，适用于大规模图，能有效捕捉邻居节点的信息。

**重要提示**： 由于整个框架基于PyG，你可以非常方便地将其他任何PyG支持的图分类模型（如GIN, HGP-SL等）应用到UPFD数据集上。

### 3. 如何参考和使用

如果你想在自己的研究或项目中使用这个仓库，建议遵循以下步骤：

#### A. 快速开始和复现
- **CodeOcean胶囊**： 作者提供了一个CodeOcean胶囊，可以一键复现结果，无需手动配置环境。这是验证模型性能最快的方式。
- **使用PyG/DGL**： 如果你想在自己的代码中快速使用数据集，最推荐的方式是直接通过PyG或DGL加载UPFD数据集，然后结合仓库中的模型代码进行训练。

#### B. 引用
如果你使用了这个仓库的数据集或代码，**必须引用**其原始论文：

```bibtex
@inproceedings{dou2021user,
  title={User Preference-aware Fake News Detection},
  author={Dou, Yingtong and Shu, Kai and Xia, Congying and Yu, Philip S. and Sun, Lichao},
  booktitle={Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  year={2021}
}
```

#### C. 贡献
作者欢迎社区贡献：
- **提交新模型**： 你可以将自己的模型代码、超参数和结果通过**创建Pull Request**的方式提交。经过验证后，你的模型会被添加到仓库中。
- **更新基准**： 新的SOTA结果会被更新到相关的基准平台（如PaperWithCode）。

### 总结

这个仓库提供了一个**标准化的假新闻检测研究平台**，其核心价值在于：
1.  **高质量数据集 (UPFD)**： 一个结构清晰、特征丰富、已被主流库采纳的基准数据集。
2.  **多种基线模型**： 实现了多个重要的GNN模型，方便研究者进行公平比较。
3.  **良好的可扩展性**： 基于PyG，易于集成新模型和进行实验。

对于研究者来说，这是一个非常理想的起点，可以专注于模型创新，而无需花费大量精力在数据构建上。

---

## 如何适配题目要求
这个代码库和数据集是完成你课题的**绝佳基础**。它几乎是为你的任务要求量身定制的。下面我们来具体分析如何利用它来完成课题的各个部分。

### 总体评估：这个代码库与课题的匹配度

**匹配度非常高。** 你的课题要求是“欺诈检测”，而该代码库专门解决“虚假信息（谣言）检测”，这是欺诈检测在社交网络中的核心应用。它直接提供了你任务b）所需的数据、特征和模型框架，并为任务a）和c）提供了坚实的基础。

---

### 具体如何利用代码库完成课题任务

#### 任务 a）可视化用户之间的社交关系

这个代码库的UPFD数据集本身就是**传播图**，可视化这些图是直接且有意义的第一步。

**你可以直接利用代码库做以下可视化：**

1.  **基础网络结构可视化**：
    - 使用 `NetworkX` 或 `PyG`/`DGL` 的内置功能，绘制几个代表性的新闻传播图（一个真新闻图，一个假新闻图）。
    - **可视化重点**：
        - **节点颜色**： 将根节点（新闻）标为一种颜色（如红色），用户节点标为另一种颜色（如蓝色）。甚至可以进一步，根据用户是否是“关键传播者”（如高PageRank值）来区分颜色。
        - **节点大小**： 根据用户的度（连接数）或PageRank值设置节点大小，直观展示核心传播者。
        - **图布局**： 使用层次化布局（如 `nx.spring_layout` 或 `nx.kamada_kawai_layout`）来清晰显示以新闻为根节点的树状传播结构。

2.  **网络统计特性可视化**：
    - 代码库没有直接提供，但你可以基于图数据轻松计算并绘制：
        - **度分布图**： 分别绘制真假新闻传播图中用户的度分布曲线。正常信息的传播可能更符合幂律分布，而谣言可能由大量低度用户（水军）推动。
        - **图直径/平均路径长度**： 比较真假新闻传播的“深度”。谣言可能传播得更深更广。
        - **社区结构**： 使用社区发现算法（如Louvain）对用户进行聚类，然后用不同颜色标注社区，观察谣言是否在特定社区内爆发式传播。

---

#### 任务 b）设计机器学习模型识别虚假信息

**这是该代码库的核心价值所在。你不需要从零开始，而是可以在一个很高的起点上进行。**

**1. 特征工程（代码库已为你完成大部分）：**
- **社交关系特征**： 图结构本身（邻接矩阵）就是最强的社交关系特征，GNN模型会自动学习。
- **用户特征**： 代码库提供了4种现成的节点特征（`bert`, `spacy`, `profile`, `content`），你可以直接使用或进行组合。这涵盖了“用户属性”。
- **传播行为特征**： 图的拓扑性质（如度中心性、聚类系数）可以作为额外的节点特征，这些可以从图中提取。

**2. 模型设计（直接使用和对比）：**
- **基线模型**： 代码库提供了6个现成的GNN模型（GCN, GAT, GraphSAGE, BiGCN等）。你可以直接运行这些模型，得到它们在数据集上的性能作为你的**基线结果**。这是你项目成果的坚实基础。
- **模型改进（你的创新点）**： 在现有模型基础上，你可以尝试以下**简单但有效的改进**，这将成为你项目的亮点：
    - **特征融合**： 尝试不同的节点特征组合（例如，将`profile`特征和`spacy`特征拼接），观察哪种特征组合对检测最有效。
    - **模型集成**： 将GCN、GAT等模型的预测结果进行投票或加权平均，看是否能提升性能。
    - **添加传统特征**： 在GNN模型的最后分类层之前，融入一些手工提取的图级特征（如整个图的节点数、边数、密度等），让模型同时学习局部和全局信息。

**3. 模型评价（标准流程）：**
- 代码库已经采用了标准的机器学习评价流程：**数据集划分（训练/验证/测试集）**。
- 你可以直接报告以下指标，这些都是学术界的标准：
    - **准确率（Accuracy）**
    - **精确率（Precision）**： 在所有被预测为假的新闻中，真正是假的比例。（重点关注）
    - **召回率（Recall）**： 在所有真正的假新闻中，被模型成功找出来的比例。（重点关注）
    - **F1-Score**： 精确率和召回率的调和平均数。
    - **AUC-ROC曲线**： 综合衡量模型性能。

---

#### 任务 c）分析传播网络的异同

在完成模型训练和可视化后，这个分析就是水到渠成的事。

**你可以从以下维度对比真假新闻的传播网络：**

1.  **网络结构特性**：
    - **图规模**： 假新闻的传播图是否平均节点数更多（病毒式传播）？还是更少（局限于小圈子）？
    - **密度**： 假新闻传播图中用户之间的互动（用户-用户边）是否更密集？（可能表明有组织的转发）
    - **深度**： 假新闻的传播路径是否更长？（可能表明经过多次转发）
    - **中心化程度**： 假新闻的传播是否更依赖于少数几个关键节点（大V）？还是更加去中心化？（用水军矩阵转发）

2.  **用户行为特性**：
    - **用户活跃度**： 传播假新闻的用户是否是新建账号？他们的平均粉丝数/关注数比是否异常？
    - **传播速度**： 通过节点时间戳（数据集提供`*_id_time_mapping.pkl`），可以分析假新闻从发布到引爆的时间模式，是否比真新闻更快？

**如何呈现：**
- 制作**对比表格**，列出上述各个维度在真假新闻群体上的平均值，并进行简单的统计检验（如T检验）说明差异是否显著。
- 将任务a）中的可视化结果并排列出，直观展示差异。

---

### 总结：你可以在代码库基础上做什么？

| 课题任务 | 代码库提供的基础 | 你可以做的创新和扩展 |
| :--- | :--- | :--- |
| **a) 可视化** | 结构化的图数据（节点、边） | 1. 实现更美观、信息量更大的可视化（如使用PyVis或Gephi）。<br>2. 计算并可视化网络统计指标（度分布、路径长度等）。 |
| **b) 模型设计** | 1. 标准数据集<br>2. 多种现成GNN模型<br>3. 丰富的节点特征 | 1. **系统性的模型对比**：公平地比较所有模型并分析其优劣。<br>2. **简单的模型改进**：如特征融合、模型集成。<br>3. **引入外部知识**：如结合情感词典分析帖子内容情感。 |
| **c) 对比分析** | 带有标签的图数据 | 1. **深入的网络测量**：超越简单可视化，进行定量的网络科学分析。<br>2. **关联模型性能**：分析为什么某些网络特征的假新闻更难被检测，从而解释模型的局限性。 |

**给你的核心建议：**

**不要只满足于复现代码库的结果。** 你的课程项目价值在于 **“在现有工作基础上的探索和分析”**。

1.  **核心工作**： 先确保能成功运行代码库中的1-2个主要模型（如UPFD-GCN和UPFD-GAT），得到可复现的基准结果。
2.  **创新点/亮点**： 然后选择上述表格中“你可以做的创新和扩展”里的一到两点进行深入。例如，**重点做一个“不同节点特征对模型性能影响”的对比实验**，这个工作量适中且分析清晰，很容易出彩。

如果你能完成“复现+对比分析+有深度的讨论”，你的项目报告将会非常出色。你现在对哪个部分最感兴趣，或者觉得哪一部分实现起来可能有困难？我们可以从那里开始详细讨论。

---
## 数据特征
是的，这篇论文在 **第3.1.2节 特征提取** 中非常详细地介绍了这些特征是如何构建的。您图片中看到的 `.npy` 文件正是这些特征处理流程的最终输出。

以下是论文中对每种特征构建方法的详细解释：

---

### 1. BERT 特征 (`new_bert_feature.npz`)
这是论文中**最重要、性能最好的特征**。

*   **目标**：捕获新闻文本和用户历史推文的**深层语义信息**。
*   **源数据**：
    *   对于**新闻节点**：使用新闻文章本身的文本。
    *   对于**用户节点**：随机抽取该用户最近的 **200条推文** 的文本内容（不包括当前传播的新闻）。
*   **构建方法**：
    1.  使用预训练的 **BERT-base-uncased** 模型。
    2.  将文本（新闻文章或200条推文的集合）输入BERT，取 `[CLS]` 标记对应的最终隐藏状态作为整个文本的表示。
    3.  得到一个 **768维** 的向量作为该节点的BERT特征。

> **简单来说**：每个节点（新闻或用户）都用一段文本代表，然后通过BERT模型将这段文本压缩成一个768维的语义向量。

---

### 2. Spacy 特征 (`new_spacy_feature.npz`)
这是一种基于**静态词向量**的特征。

*   **目标**：提供一种基于传统词嵌入的语义表示，与BERT的动态上下文表示形成对比。
*   **源数据**：与BERT特征相同（新闻文章或用户200条推文）。
*   **构建方法**：
    1.  使用 `spaCy` 库中的预训练 **`en_core_web_lg`** 模型，该模型提供了300维的GloVe词向量。
    2.  对输入文本进行分词，获取每个词的300维向量。
    3.  对所有词的词向量取**平均值**，得到一个 **300维** 的向量作为该节点的Spacy特征。

> **简单来说**：将文本中每个词的意思“平均”一下，得到一个300维的向量。

---

### 3. Profile 特征 (`new_profile_feature.npz`)
这是基于用户**元数据**的数值型特征。

*   **目标**：捕获用户的**社交背景和活跃度**等统计信息。
*   **源数据**：从Twitter API获取的**用户个人资料**。
*   **构建方法**：
    论文提取了 **10个** 用户档案特征，包括：
    *   `statuses_count`：用户发出的推文/转推总数。
    *   `followers_count`：粉丝数。
    *   `friends_count`：关注数。
    *   `favourites_count`：点赞数。
    *   `listed_count`：被列入清单的次数。
    *   `account_creation_date`：账户创建时间（可能转换为账户年龄）。
    *   `account_verified`：是否认证（布尔值）。
    *   `default_profile` / `default_profile_image`：是否使用默认配置/头像（可能表示不活跃或机器人）。
    *   等等。
    这些特征被组合成一个 **10维** 的向量。

> **简单来说**：不看用户发了什么内容，只看他的社交数据（如粉丝数、发帖数等），形成一个10维的档案向量。

---

### 4. Content 特征 (`new_content_feature.npz`)
这是一个**混合特征**。

*   **目标**：结合当前的转发评论和简单的档案信息。
*   **构建方法**：
    论文直接将 **Spacy特征（300维）** 和 **Profile特征（10维）** 连接在一起，形成一个 **310维** 的向量。
    *   `Content特征 = [Spacy特征 (300维) ; Profile特征 (10维)]`

---

## 关于GCN
好的，这是一个非常核心的问题。标准GCN（图卷积网络）是图神经网络领域的奠基性工作之一。要理解它，关键在于弄明白它如何将传统卷积（用于图像等欧几里得数据）的思想巧妙地推广到图（非欧几里得数据）这种结构上。

下面我将分步解析，并阐述其与卷积的关系。

### 1. 什么是标准GCN？

标准GCN，通常指的是Thomas Kipf和Max Welling在2017年提出的**半监督图卷积网络**。它的核心目标是为图中的每个节点学习一个有效的特征表示，这个表示不仅包含节点自身的特征，还聚合了其邻居节点的信息。

一个典型的GCN层的前向传播公式如下：

$H^{(l+1)} = \sigma\left( \tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} H^{(l)} W^{(l)} \right)$

我们来拆解这个公式：
*   $H^{(l)}$：第 $l$ 层的节点特征表示。$H^{(0)}$ 就是输入的节点特征矩阵 $X$。
*   $\tilde{A} = A + I$：邻接矩阵 $A$ 加上自环（单位矩阵 $I$）。这确保了每个节点在聚合信息时也会考虑自身的特征。
*   $\tilde{D}$：是 $\tilde{A}$ 的度矩阵（对角矩阵），其对角线元素 $\tilde{D}_{ii} = \sum_j \tilde{A}_{ij}$，表示每个节点的度（连接数）。
*   $\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}}$：这是**对称归一化拉普拉斯矩阵**的一种近似。它的作用是对邻接矩阵进行归一化，防止节点度数的差异导致特征在传播过程中尺度发生剧烈变化。
*   $W^{(l)}$：第 $l$ 层可学习的权重参数。
*   $\sigma$：非线性激活函数，如ReLU。

**简单来说，每一层GCN都在做一件事：对每个节点，将其自身的特征和所有邻居节点的特征进行加权平均，然后通过一个可学习的权重矩阵进行变换。**

### 2. GCN与卷积的“关系”：一种思想上的推广

GCN并不是直接在图上定义了一个像图像卷积那样直观的“卷积核”。它们的关系更多是**思想上的继承和推广**，即**局部连接**和**权重共享**。

#### 在图像卷积中（传统卷积）：
*   **局部连接**：一个像素点的输出值，只由其周围一个固定大小的局部区域（即卷积核覆盖的区域）的像素决定。
*   **权重共享**：同一个卷积核会滑过整张图像，意味着无论这个局部区域在图像的哪个位置，都使用相同的权重参数进行计算。


#### 在GCN中（图卷积）：
*   **局部连接**：一个节点的输出特征，由其**一阶邻居**（直接相连的节点）决定。你可以把每个节点的邻居集合看作是它在图上的“局部感受野”。与图像中固定的、规则的邻域不同，图中的邻域大小和结构是**不规则**的。
*   **权重共享**：GCN层中的权重矩阵 \(W^{(l)}\) 被应用于图中的**每一个节点**。这意味着，所有节点都使用同一套参数来变换和聚合来自其邻居的信息。这实现了类似权重共享的效果，大大减少了参数量，使模型能够处理大规模图。

### 总结：二者的核心区别与联系

| 特性 | 传统图像卷积 | 标准图卷积 (GCN) |
| :--- | :--- | :--- |
| **数据基础** | 规整的网格结构（如图像像素） | 不规则的图结构 |
| **局部感受野** | 固定大小、形状规则的滑动窗口（如3x3） | 节点的**一阶邻居**，大小和连接关系可变 |
| **权重共享** | 卷积核在规则网格上滑动，共享权重 | 同一个权重矩阵 \(W\) 应用于所有节点 |
| **核心思想** | 聚合局部邻域信息以提取特征 | **聚合局部邻域信息以提取特征** |

因此，GCN可以理解为**将卷积的“局部聚合”思想从规则的欧几里得域推广到了不规则的非欧几里得域（图）**。它不是严格数学意义上的卷积，而是一种功能上的模拟和扩展，是处理图结构数据的一种强大工具。

在你之前看到的UPFD框架中，GCN（或其变体GAT、GraphSAGE）扮演的就是这个“图编码器”的角色，它通过多层卷积操作，将每个新闻传播图中所有用户节点的信息逐步聚合，最终形成一个能代表整个图（即整个新闻事件）的嵌入表示，用于假新闻分类。